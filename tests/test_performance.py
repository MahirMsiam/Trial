"""
Performance Tests

Tests for measuring and validating system performance including
latency, cache effectiveness, and resource usage.
"""

import pytest
import time
import sys
import os
from typing import List
import statistics

# Add parent directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Performance thresholds (in seconds)
SEMANTIC_SEARCH_THRESHOLD = 0.5  # 500ms
KEYWORD_SEARCH_THRESHOLD = 0.2   # 200ms
HYBRID_SEARCH_THRESHOLD = 1.0    # 1 second
LLM_GENERATION_THRESHOLD = 5.0   # 5 seconds
END_TO_END_THRESHOLD = 6.0       # 6 seconds


@pytest.fixture(scope="module")
def rag_pipeline():
    """Initialize RAG pipeline for testing"""
    try:
        from rag_pipeline import RAGPipeline
        pipeline = RAGPipeline()
        return pipeline
    except Exception as e:
        pytest.skip(f"Could not initialize RAG pipeline: {e}")


@pytest.fixture(scope="module")
def retriever():
    """Initialize retriever for testing"""
    try:
        from rag_retriever import HybridRetriever
        retriever = HybridRetriever()
        return retriever
    except Exception as e:
        pytest.skip(f"Could not initialize retriever: {e}")


def measure_execution_time(func, *args, **kwargs):
    """Measure function execution time"""
    start = time.time()
    result = func(*args, **kwargs)
    end = time.time()
    return result, end - start


def run_multiple_times(func, n: int, *args, **kwargs) -> List[float]:
    """Run function multiple times and collect execution times"""
    times = []
    for _ in range(n):
        _, exec_time = measure_execution_time(func, *args, **kwargs)
        times.append(exec_time)
    return times


def calculate_percentiles(times: List[float]) -> dict:
    """Calculate performance percentiles"""
    if not times:
        return {}
    
    sorted_times = sorted(times)
    return {
        'avg': statistics.mean(times),
        'median': statistics.median(times),
        'p50': sorted_times[len(sorted_times) // 2],
        'p95': sorted_times[int(len(sorted_times) * 0.95)],
        'p99': sorted_times[int(len(sorted_times) * 0.99)],
        'min': min(times),
        'max': max(times)
    }


# Latency Tests
def test_semantic_search_latency(retriever):
    """Measure semantic search latency"""
    query = "What are the legal requirements for filing a writ petition?"
    
    try:
        times = run_multiple_times(retriever.retrieve_semantic, 20, query, top_k=10)
        percentiles = calculate_percentiles(times)
        
        print(f"\nSemantic Search Performance:")
        print(f"  Average: {percentiles['avg']:.3f}s")
        print(f"  P50: {percentiles['p50']:.3f}s")
        print(f"  P95: {percentiles['p95']:.3f}s")
        print(f"  P99: {percentiles['p99']:.3f}s")
        
        # Assert P95 is within threshold
        assert percentiles['p95'] < SEMANTIC_SEARCH_THRESHOLD, \
            f"P95 latency {percentiles['p95']:.3f}s exceeds threshold {SEMANTIC_SEARCH_THRESHOLD}s"
    except Exception as e:
        pytest.skip(f"Semantic search not available: {e}")


def test_keyword_search_latency(retriever):
    """Measure keyword search latency"""
    query = "murder cases"
    filters = {"case_type": "Criminal Appeal"}
    
    times = run_multiple_times(retriever.retrieve_keyword, 50, query, filters, limit=10)
    percentiles = calculate_percentiles(times)
    
    print(f"\nKeyword Search Performance:")
    print(f"  Average: {percentiles['avg']:.3f}s")
    print(f"  P50: {percentiles['p50']:.3f}s")
    print(f"  P95: {percentiles['p95']:.3f}s")
    
    assert percentiles['p95'] < KEYWORD_SEARCH_THRESHOLD, \
        f"P95 latency {percentiles['p95']:.3f}s exceeds threshold {KEYWORD_SEARCH_THRESHOLD}s"


def test_hybrid_search_latency(retriever):
    """Measure hybrid search latency"""
    query = "cases related to Section 302 IPC"
    
    try:
        times = run_multiple_times(retriever.hybrid_retrieve, 20, query, top_k=10, filters={})
        percentiles = calculate_percentiles(times)
        
        print(f"\nHybrid Search Performance:")
        print(f"  Average: {percentiles['avg']:.3f}s")
        print(f"  P50: {percentiles['p50']:.3f}s")
        print(f"  P95: {percentiles['p95']:.3f}s")
        
        assert percentiles['p95'] < HYBRID_SEARCH_THRESHOLD, \
            f"P95 latency {percentiles['p95']:.3f}s exceeds threshold {HYBRID_SEARCH_THRESHOLD}s"
    except Exception as e:
        pytest.skip(f"Hybrid search not available: {e}")


def test_llm_generation_latency(rag_pipeline):
    """Measure LLM generation latency"""
    query = "What is the definition of murder under IPC?"
    
    try:
        times = run_multiple_times(rag_pipeline.process_query, 10, query, filters={})
        percentiles = calculate_percentiles(times)
        
        print(f"\nLLM Generation Performance:")
        print(f"  Average: {percentiles['avg']:.3f}s")
        print(f"  P50: {percentiles['p50']:.3f}s")
        print(f"  P95: {percentiles['p95']:.3f}s")
        
        assert percentiles['p95'] < LLM_GENERATION_THRESHOLD, \
            f"P95 latency {percentiles['p95']:.3f}s exceeds threshold {LLM_GENERATION_THRESHOLD}s"
    except Exception as e:
        pytest.skip(f"LLM generation not available: {e}")


def test_end_to_end_latency(rag_pipeline):
    """Measure full RAG pipeline latency"""
    query = "Explain the legal procedure for bail applications"
    
    try:
        times = run_multiple_times(rag_pipeline.process_query, 10, query, filters={})
        percentiles = calculate_percentiles(times)
        
        print(f"\nEnd-to-End Performance:")
        print(f"  Average: {percentiles['avg']:.3f}s")
        print(f"  P50: {percentiles['p50']:.3f}s")
        print(f"  P95: {percentiles['p95']:.3f}s")
        
        assert percentiles['p95'] < END_TO_END_THRESHOLD, \
            f"P95 latency {percentiles['p95']:.3f}s exceeds threshold {END_TO_END_THRESHOLD}s"
    except Exception as e:
        pytest.skip(f"End-to-end test not available: {e}")


# Cache Performance Tests
@pytest.mark.skipif(not os.getenv('CACHE_ENABLED', 'true').lower() == 'true',
                    reason="Caching not enabled")
def test_cache_hit_rate(rag_pipeline):
    """Measure cache effectiveness"""
    queries = [
        "What is Section 302?",
        "Explain bail procedures",
        "What is Section 302?",  # Repeat
        "Murder case laws",
        "Explain bail procedures",  # Repeat
    ]
    
    try:
        # Execute queries
        for query in queries:
            rag_pipeline.process_query(query, filters={})
        
        # Get cache stats
        stats = rag_pipeline.get_cache_stats()
        
        print(f"\nCache Performance:")
        for cache_name, cache_stats in stats.items():
            if isinstance(cache_stats, dict) and 'hit_rate' in cache_stats:
                print(f"  {cache_name}: {cache_stats['hit_rate']:.1f}% hit rate")
                print(f"    Hits: {cache_stats.get('hits', 0)}, Misses: {cache_stats.get('misses', 0)}")
        
        # We expect at least 2 cache hits from repeated queries
        # This is a weak assertion since cache behavior depends on configuration
        total_hits = sum(s.get('hits', 0) for s in stats.values() if isinstance(s, dict))
        print(f"  Total cache hits: {total_hits}")
        
    except AttributeError:
        pytest.skip("Cache stats not available")


@pytest.mark.skipif(not os.getenv('CACHE_ENABLED', 'true').lower() == 'true',
                    reason="Caching not enabled")
def test_cache_speedup(rag_pipeline):
    """Measure speedup from caching"""
    query = "What are the requirements for filing a writ petition?"
    
    try:
        # First query (cache miss)
        _, time_uncached = measure_execution_time(
            rag_pipeline.process_query, query, filters={}
        )
        
        # Second query (should be cached)
        _, time_cached = measure_execution_time(
            rag_pipeline.process_query, query, filters={}
        )
        
        speedup = time_uncached / time_cached if time_cached > 0 else 1.0
        
        print(f"\nCache Speedup:")
        print(f"  Uncached: {time_uncached:.3f}s")
        print(f"  Cached: {time_cached:.3f}s")
        print(f"  Speedup: {speedup:.2f}x")
        
        # We expect at least 2x speedup from caching
        # This is optimistic and may not always be true
        if speedup > 1.5:
            print("  âœ“ Significant speedup from caching!")
        
    except Exception as e:
        pytest.skip(f"Cache speedup test failed: {e}")


# Database Performance Tests
def test_database_query_time():
    """Measure database query execution time"""
    import sqlite3
    from config import DATABASE_PATH
    
    if not os.path.exists(DATABASE_PATH):
        pytest.skip("Database not found")
    
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    
    queries = [
        "SELECT * FROM judgments WHERE case_type = 'Criminal Appeal' LIMIT 10",
        "SELECT * FROM judgments WHERE case_year = 2023 LIMIT 10",
        "SELECT * FROM judgments WHERE full_text LIKE '%murder%' LIMIT 10",
    ]
    
    print(f"\nDatabase Query Performance:")
    for query in queries:
        times = []
        for _ in range(10):
            start = time.time()
            cursor.execute(query)
            cursor.fetchall()
            end = time.time()
            times.append(end - start)
        
        avg_time = statistics.mean(times)
        print(f"  {query[:50]}...")
        print(f"    Average: {avg_time:.4f}s")
        
        # Database queries should be fast
        assert avg_time < 0.1, f"Query too slow: {avg_time:.4f}s"
    
    conn.close()


# Memory Usage Tests
def test_memory_usage():
    """Measure memory consumption"""
    try:
        import psutil
        process = psutil.Process()
        
        # Get initial memory
        mem_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # Initialize components
        from rag_pipeline import RAGPipeline
        pipeline = RAGPipeline()
        
        # Run some queries
        for i in range(10):
            pipeline.process_query(f"Test query {i}", filters={})
        
        # Get final memory
        mem_after = process.memory_info().rss / 1024 / 1024  # MB
        mem_increase = mem_after - mem_before
        
        print(f"\nMemory Usage:")
        print(f"  Before: {mem_before:.2f} MB")
        print(f"  After: {mem_after:.2f} MB")
        print(f"  Increase: {mem_increase:.2f} MB")
        
        # Memory should not grow excessively (less than 1GB increase)
        assert mem_increase < 1024, f"Excessive memory increase: {mem_increase:.2f} MB"
        
    except ImportError:
        pytest.skip("psutil not installed")


# Concurrency Tests
def test_concurrent_search():
    """Test concurrent search requests"""
    import threading
    from rag_retriever import HybridRetriever
    
    try:
        retriever = HybridRetriever()
        
        results = []
        errors = []
        
        def search_task(query_id):
            try:
                start = time.time()
                retriever.retrieve_keyword(f"test query {query_id}", {}, limit=5)
                end = time.time()
                results.append(end - start)
            except Exception as e:
                errors.append(str(e))
        
        # Create 20 concurrent threads
        threads = []
        for i in range(20):
            thread = threading.Thread(target=search_task, args=(i,))
            threads.append(thread)
            thread.start()
        
        # Wait for all threads
        for thread in threads:
            thread.join()
        
        print(f"\nConcurrent Search Performance:")
        print(f"  Successful requests: {len(results)}")
        print(f"  Failed requests: {len(errors)}")
        
        if results:
            percentiles = calculate_percentiles(results)
            print(f"  Average latency: {percentiles['avg']:.3f}s")
            print(f"  P95 latency: {percentiles['p95']:.3f}s")
        
        # All requests should succeed
        assert len(errors) == 0, f"Concurrent requests failed: {errors}"
        
    except Exception as e:
        pytest.skip(f"Concurrent test failed: {e}")


# Ranking Performance Tests
@pytest.mark.skipif(not os.getenv('USE_BM25', 'false').lower() == 'true',
                    reason="BM25 not enabled")
def test_bm25_ranking_time():
    """Measure BM25 computation time"""
    from ranking_algorithms import BM25Ranker
    
    # Create sample corpus
    corpus = [f"Sample document {i} with legal text" for i in range(100)]
    
    start = time.time()
    ranker = BM25Ranker(corpus)
    init_time = time.time() - start
    
    # Measure ranking time
    query = "legal text"
    documents = [{'id': i, 'full_text': doc} for i, doc in enumerate(corpus)]
    
    start = time.time()
    ranker.rank_documents(query, documents)
    rank_time = time.time() - start
    
    print(f"\nBM25 Performance:")
    print(f"  Initialization: {init_time:.3f}s")
    print(f"  Ranking 100 docs: {rank_time:.3f}s")
    
    # Should be reasonably fast
    assert rank_time < 1.0, f"BM25 ranking too slow: {rank_time:.3f}s"


@pytest.mark.skipif(not os.getenv('USE_RRF', 'false').lower() == 'true',
                    reason="RRF not enabled")
def test_rrf_fusion_time():
    """Measure RRF fusion time"""
    from ranking_algorithms import ReciprocalRankFusion
    
    rrf = ReciprocalRankFusion(k=60)
    
    # Create sample ranked lists
    list1 = [{'id': i, 'score': 10 - i} for i in range(50)]
    list2 = [{'id': i, 'score': 15 - i} for i in range(50)]
    list3 = [{'id': i, 'score': 12 - i} for i in range(50)]
    
    start = time.time()
    fused = rrf.fuse([list1, list2, list3])
    fusion_time = time.time() - start
    
    print(f"\nRRF Performance:")
    print(f"  Fusing 3 lists of 50 items: {fusion_time:.4f}s")
    
    # Should be very fast
    assert fusion_time < 0.1, f"RRF fusion too slow: {fusion_time:.4f}s"


if __name__ == '__main__':
    pytest.main([__file__, '-v', '-s'])
