# LLM API Keys (at least one is required)
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# LLM Provider Configuration
LLM_PROVIDER=openai
# Options: openai, anthropic, local
LLM_MODEL=gpt-4-turbo-preview
# For OpenAI: gpt-4-turbo-preview, gpt-3.5-turbo
# For Anthropic: claude-3-sonnet-20240229, claude-3-opus-20240229

# LLM Generation Parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

# Optional: Local Model Configuration (for local LLM)
LOCAL_LLM_ENDPOINT=http://localhost:8000
LOCAL_LLM_MODEL=llama-2-7b-chat

# Retrieval Configuration
TOP_K_CHUNKS=5
TOP_K_SQL_RESULTS=10
SIMILARITY_THRESHOLD=0.7
HYBRID_WEIGHT_SEMANTIC=0.6
HYBRID_WEIGHT_KEYWORD=0.4

# Chat Configuration
MAX_HISTORY_TURNS=5
MAX_CONTEXT_LENGTH=4000
SESSION_TIMEOUT=3600

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/rag_system.log

# Database Path (optional override)
# DATABASE_PATH=extracted_data/database.db

# ============================================
# FASTAPI BACKEND CONFIGURATION
# ============================================

# API Server Settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4  # Number of uvicorn workers for production

# CORS Settings (comma-separated list of allowed origins)
# IMPORTANT: Using * with allow_credentials=True may fail in some browsers
# For production, always specify exact origins like: https://yourdomain.com,https://app.yourdomain.com
# For development with credentials, use: http://localhost:3000,http://localhost:5173
CORS_ORIGINS=*
# Production example: CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# API Security (future use)
# API_KEY_ENABLED=false
# API_KEY_HEADER=X-API-Key

# Rate Limiting (future use)
# RATE_LIMIT_ENABLED=false
# RATE_LIMIT_REQUESTS=100
# RATE_LIMIT_WINDOW=60

# ============================================
# SETUP INSTRUCTIONS:
# ============================================
# 1. Copy this file to .env: copy .env.example .env
# 2. Fill in your actual API keys above
# 3. Choose your LLM provider (openai or anthropic)
# 4. Never commit .env to version control
# 5. At least one LLM provider key is required
# 6. Configure API settings if running the FastAPI backend
#    - API_HOST and API_PORT can be customized for deployment
