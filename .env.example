# LLM API Keys (at least one is required)
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# LLM Provider Configuration
LLM_PROVIDER=openai
# Options: openai, anthropic, local
LLM_MODEL=gpt-4-turbo-preview
# For OpenAI: gpt-4-turbo-preview, gpt-3.5-turbo
# For Anthropic: claude-3-sonnet-20240229, claude-3-opus-20240229

# LLM Generation Parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

# Optional: Local Model Configuration (for local LLM)
LOCAL_LLM_ENDPOINT=http://localhost:8000
LOCAL_LLM_MODEL=llama-2-7b-chat

# Retrieval Configuration
TOP_K_CHUNKS=5
TOP_K_SQL_RESULTS=10
SIMILARITY_THRESHOLD=0.7
HYBRID_WEIGHT_SEMANTIC=0.6
HYBRID_WEIGHT_KEYWORD=0.4

# Chat Configuration
MAX_HISTORY_TURNS=5
MAX_CONTEXT_LENGTH=4000
SESSION_TIMEOUT=3600

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/rag_system.log

# Database Path (optional override)
# DATABASE_PATH=extracted_data/database.db

# ============================================
# SETUP INSTRUCTIONS:
# ============================================
# 1. Copy this file to .env: copy .env.example .env
# 2. Fill in your actual API keys above
# 3. Choose your LLM provider (openai or anthropic)
# 4. Never commit .env to version control
# 5. At least one LLM provider key is required
