# LLM API Keys (at least one is required)
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
GEMINI_API_KEY=your-gemini-api-key-here

# LLM Provider Configuration
LLM_PROVIDER=openai
# Options: openai, anthropic, gemini, local
LLM_MODEL=gpt-4-turbo-preview
# For OpenAI: gpt-4-turbo-preview, gpt-3.5-turbo
# For Anthropic: claude-3-sonnet-20240229, claude-3-opus-20240229
# For Gemini: gemini-pro, gemini-1.5-pro

# LLM Generation Parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=2000

# Optional: Local Model Configuration (for local LLM)
LOCAL_LLM_ENDPOINT=http://localhost:8000
LOCAL_LLM_MODEL=llama-2-7b-chat

# Gemini Configuration (when using gemini provider)
# GEMINI_MODEL=gemini-1.5-pro
# gemini-pro: Faster, lower cost | gemini-1.5-pro: More capable, larger context

# Retrieval Configuration
TOP_K_CHUNKS=5
TOP_K_SQL_RESULTS=10
SIMILARITY_THRESHOLD=0.7
HYBRID_WEIGHT_SEMANTIC=0.6
HYBRID_WEIGHT_KEYWORD=0.4

# Chat Configuration
MAX_HISTORY_TURNS=5
MAX_CONTEXT_LENGTH=4000
SESSION_TIMEOUT=3600

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/rag_system.log

# Database Path (optional override)
# DATABASE_PATH=extracted_data/database.db

# ============================================
# FASTAPI BACKEND CONFIGURATION
# ============================================

# API Server Settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4  # Number of uvicorn workers for production

# CORS Settings (comma-separated list of allowed origins)
# IMPORTANT: Using * with allow_credentials=True may fail in some browsers
# For production, always specify exact origins like: https://yourdomain.com,https://app.yourdomain.com
# For development with credentials, use: http://localhost:3000,http://localhost:5173
CORS_ORIGINS=*
# Production example: CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# API Security (future use)
# API_KEY_ENABLED=false
# API_KEY_HEADER=X-API-Key

# Rate Limiting (future use)
# RATE_LIMIT_ENABLED=false
# RATE_LIMIT_REQUESTS=100
# RATE_LIMIT_WINDOW=60

# ============================================
# CACHING CONFIGURATION
# ============================================

# Enable/disable caching (true/false)
CACHE_ENABLED=true

# Cache backend: 'memory' (single worker) or 'redis' (multi-worker)
CACHE_BACKEND=memory

# Cache TTL (time-to-live) in seconds
CACHE_TTL_QUERY=3600      # Query results: 1 hour
CACHE_TTL_LLM=7200        # LLM responses: 2 hours
CACHE_TTL_EMBEDDING=86400 # Embeddings: 24 hours

# Redis connection (only if CACHE_BACKEND=redis)
# REDIS_URL=redis://localhost:6379/0
# For production with password: redis://:password@localhost:6379/0

# ============================================
# ADVANCED RANKING (EXPERIMENTAL)
# ============================================

# Enable BM25 ranking for keyword search (true/false)
USE_BM25=false

# Enable Reciprocal Rank Fusion for combining results (true/false)
USE_RRF=false

# BM25 parameters (only if USE_BM25=true)
BM25_K1=1.5  # Term frequency saturation (1.2-2.0)
BM25_B=0.75  # Length normalization (0.0-1.0)

# RRF parameter (only if USE_RRF=true)
RRF_K=60     # RRF constant (typically 60)

# ============================================
# PERFORMANCE MONITORING (OPTIONAL)
# ============================================

# Enable Prometheus metrics endpoint
ENABLE_METRICS=false
METRICS_PORT=9090

# ============================================
# DATABASE OPTIMIZATION (OPTIONAL)
# ============================================

# Enable connection pooling for database (experimental)
USE_CONNECTION_POOL=false

# ============================================
# SETUP INSTRUCTIONS:
# ============================================
# 1. Copy this file to .env: copy .env.example .env
# 2. Fill in your actual API keys above
# 3. Choose your LLM provider (openai, anthropic, or gemini)
# 4. Never commit .env to version control
# 5. At least one LLM provider key is required
# 6. Configure API settings if running the FastAPI backend
#    - API_HOST and API_PORT can be customized for deployment
# 7. For production:
#    - Enable caching (CACHE_ENABLED=true)
#    - Use Redis for multi-worker deployments (CACHE_BACKEND=redis)
#    - Consider enabling advanced ranking for better results
#    - Run database optimization: python database_optimizer.py optimize
